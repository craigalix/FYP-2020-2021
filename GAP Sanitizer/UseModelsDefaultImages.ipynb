{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import backend as k\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Activation, Add, Dense, Flatten, Concatenate, Dropout, \\\n",
    "    Reshape, Multiply, Lambda, Subtract, ReLU, Conv2DTranspose, LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[<KerasTensor: shape=(None, 16, 16, 1) dtype=float32 (created by layer 'input_1')>, <KerasTensor: shape=(None, 64, 64, 1) dtype=float32 (created by layer 'input_2')>]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name=None), name='conv2d_transpose_1/Sigmoid:0', description=\"created by layer 'conv2d_transpose_1'\")\n",
      "load_data()\n",
      "\n",
      "Amount of images loaded =  21814\n",
      "input shape = (1, 64, 64, 1)\n",
      "noise(1, 16, 16, 1)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CCE7BEF948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "output shape = (1, 64, 64, 1)\n",
      "output shape after = (64, 64)\n"
     ]
    }
   ],
   "source": [
    "##### sanitizer.h5 #####\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "index = 136000;\n",
    "# load model\n",
    "model = load_model('./v3_Output/v3_1_1_4/1_1_4_'+str(index)+'_sanitizer.h5')\n",
    "\n",
    "# show model i/o\n",
    "input = model.input\n",
    "output = model.output\n",
    "print (input)\n",
    "print (output)\n",
    "\n",
    "\n",
    "imageindex = 6070# here to change image\n",
    "\n",
    "# summarize model.\n",
    "# model.summary()\n",
    "\n",
    "def load_data():\n",
    "    print(\"load_data()\\n\")\n",
    "    d = loadmat('./UTKFace_Gray_AgeAbove3.mat')\n",
    "    image, gender, age = d[\"image\"], d[\"gender\"][0], d[\"age\"][0]\n",
    "    print(\"Amount of images loaded = \", len(image))\n",
    "    idx = np.where((age > 50) | (age < 20))[0]\n",
    "    image = image[idx] / 255.0\n",
    "    gender = gender[idx]\n",
    "    age = age[idx]\n",
    "\n",
    "    age = np.where(age > 30, 1, 0)\n",
    "    age = np_utils.to_categorical(age, 2)\n",
    "    gender = np_utils.to_categorical(gender, 2)\n",
    "\n",
    "    return image, gender, age\n",
    "\n",
    "# load dataset\n",
    "image, gender, age = load_data()\n",
    "img = image[imageindex : imageindex + 1]\n",
    "print(\"input shape = \"+str(img.shape) )\n",
    "# imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "copyImg = img[0].copy()\n",
    "resizeImg = copyImg.resize((200,200))\n",
    "cv2.imshow(\"3\", img[0])\n",
    "\n",
    "\n",
    "# cv2.imshow(\"7\", resizeImg)\n",
    "# print(\"shape\"+str(imgGray.shape))\n",
    "# imgGray = cv2.resize(imgGray,(64,64))\n",
    "# imgResize = imgGray.reshape(1,64,64,1) # see imgResize\n",
    "# print(\"resize\"+str(imgResize.shape))\n",
    "# cv2.imshow(\"1\", imgGray)\n",
    "\n",
    "# create noise\n",
    "#noise_in = np.random.normal(0, 1, (1, 64, 64, 1))\n",
    "noise_in = np.random.normal(0, 1, (1, 16, 16, 1))\n",
    "# noise_pic = noise_in.reshape(22,22)\n",
    "# cv2.imshow(\"2\", noise_pic)\n",
    "\n",
    "print(\"noise\"+str(noise_in.shape))\n",
    "\n",
    "# evaluate the model\n",
    "score = model.predict([noise_in,img])\n",
    "print (\"output shape = \"+str(score.shape))\n",
    "# cropped = score[0,0:64,0:64,0]\n",
    "OUTPUT = score.reshape(64,64)\n",
    "# resizeOUTPUT = OUTPUT.resize(200,200)\n",
    "print (\"output shape after = \"+str(OUTPUT.shape))\n",
    "# cropped = cv2.resize(cropped,(64,64))\n",
    "# OUTPUT = cv2.resize(OUTPUT,(600,600))\n",
    "\n",
    "#debug\n",
    "# print(noise_in)\n",
    "# cv2.imshow('3', cropped)\n",
    "cv2.imshow('sample image', OUTPUT)\n",
    "# cv2.imshow('resized image', resizeOUTPUT)\n",
    "\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "array = [[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]]\n",
    "array = np.array(array)\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9, 4)\n",
      "(18, 18, 1)\n",
      "54.0\n",
      "[[[  0.]\n",
      "  [  1.]\n",
      "  [  2.]\n",
      "  [  3.]\n",
      "  [  4.]\n",
      "  [  5.]\n",
      "  [  6.]\n",
      "  [  7.]\n",
      "  [  8.]\n",
      "  [  9.]\n",
      "  [ 10.]\n",
      "  [ 11.]\n",
      "  [ 12.]\n",
      "  [ 13.]\n",
      "  [ 14.]\n",
      "  [ 15.]\n",
      "  [ 16.]\n",
      "  [ 17.]]\n",
      "\n",
      " [[ 18.]\n",
      "  [ 19.]\n",
      "  [ 20.]\n",
      "  [ 21.]\n",
      "  [ 22.]\n",
      "  [ 23.]\n",
      "  [ 24.]\n",
      "  [ 25.]\n",
      "  [ 26.]\n",
      "  [ 27.]\n",
      "  [ 28.]\n",
      "  [ 29.]\n",
      "  [ 30.]\n",
      "  [ 31.]\n",
      "  [ 32.]\n",
      "  [ 33.]\n",
      "  [ 34.]\n",
      "  [ 35.]]\n",
      "\n",
      " [[ 36.]\n",
      "  [ 37.]\n",
      "  [ 38.]\n",
      "  [ 39.]\n",
      "  [ 40.]\n",
      "  [ 41.]\n",
      "  [ 42.]\n",
      "  [ 43.]\n",
      "  [ 44.]\n",
      "  [ 45.]\n",
      "  [ 46.]\n",
      "  [ 47.]\n",
      "  [ 48.]\n",
      "  [ 49.]\n",
      "  [ 50.]\n",
      "  [ 51.]\n",
      "  [ 52.]\n",
      "  [ 53.]]\n",
      "\n",
      " [[ 54.]\n",
      "  [ 55.]\n",
      "  [ 56.]\n",
      "  [ 57.]\n",
      "  [ 58.]\n",
      "  [ 59.]\n",
      "  [ 60.]\n",
      "  [ 61.]\n",
      "  [ 62.]\n",
      "  [ 63.]\n",
      "  [ 64.]\n",
      "  [ 65.]\n",
      "  [ 66.]\n",
      "  [ 67.]\n",
      "  [ 68.]\n",
      "  [ 69.]\n",
      "  [ 70.]\n",
      "  [ 71.]]\n",
      "\n",
      " [[ 72.]\n",
      "  [ 73.]\n",
      "  [ 74.]\n",
      "  [ 75.]\n",
      "  [ 76.]\n",
      "  [ 77.]\n",
      "  [ 78.]\n",
      "  [ 79.]\n",
      "  [ 80.]\n",
      "  [ 81.]\n",
      "  [ 82.]\n",
      "  [ 83.]\n",
      "  [ 84.]\n",
      "  [ 85.]\n",
      "  [ 86.]\n",
      "  [ 87.]\n",
      "  [ 88.]\n",
      "  [ 89.]]\n",
      "\n",
      " [[ 90.]\n",
      "  [ 91.]\n",
      "  [ 92.]\n",
      "  [ 93.]\n",
      "  [ 94.]\n",
      "  [ 95.]\n",
      "  [ 96.]\n",
      "  [ 97.]\n",
      "  [ 98.]\n",
      "  [ 99.]\n",
      "  [100.]\n",
      "  [101.]\n",
      "  [102.]\n",
      "  [103.]\n",
      "  [104.]\n",
      "  [105.]\n",
      "  [106.]\n",
      "  [107.]]\n",
      "\n",
      " [[108.]\n",
      "  [109.]\n",
      "  [110.]\n",
      "  [111.]\n",
      "  [112.]\n",
      "  [113.]\n",
      "  [114.]\n",
      "  [115.]\n",
      "  [116.]\n",
      "  [117.]\n",
      "  [118.]\n",
      "  [119.]\n",
      "  [120.]\n",
      "  [121.]\n",
      "  [122.]\n",
      "  [123.]\n",
      "  [124.]\n",
      "  [125.]]\n",
      "\n",
      " [[126.]\n",
      "  [127.]\n",
      "  [128.]\n",
      "  [129.]\n",
      "  [130.]\n",
      "  [131.]\n",
      "  [132.]\n",
      "  [133.]\n",
      "  [134.]\n",
      "  [135.]\n",
      "  [136.]\n",
      "  [137.]\n",
      "  [138.]\n",
      "  [139.]\n",
      "  [140.]\n",
      "  [141.]\n",
      "  [142.]\n",
      "  [143.]]\n",
      "\n",
      " [[144.]\n",
      "  [145.]\n",
      "  [146.]\n",
      "  [147.]\n",
      "  [148.]\n",
      "  [149.]\n",
      "  [150.]\n",
      "  [151.]\n",
      "  [152.]\n",
      "  [153.]\n",
      "  [154.]\n",
      "  [155.]\n",
      "  [156.]\n",
      "  [157.]\n",
      "  [158.]\n",
      "  [159.]\n",
      "  [160.]\n",
      "  [161.]]\n",
      "\n",
      " [[162.]\n",
      "  [163.]\n",
      "  [164.]\n",
      "  [165.]\n",
      "  [166.]\n",
      "  [167.]\n",
      "  [168.]\n",
      "  [169.]\n",
      "  [170.]\n",
      "  [171.]\n",
      "  [172.]\n",
      "  [173.]\n",
      "  [174.]\n",
      "  [175.]\n",
      "  [176.]\n",
      "  [177.]\n",
      "  [178.]\n",
      "  [179.]]\n",
      "\n",
      " [[180.]\n",
      "  [181.]\n",
      "  [182.]\n",
      "  [183.]\n",
      "  [184.]\n",
      "  [185.]\n",
      "  [186.]\n",
      "  [187.]\n",
      "  [188.]\n",
      "  [189.]\n",
      "  [190.]\n",
      "  [191.]\n",
      "  [192.]\n",
      "  [193.]\n",
      "  [194.]\n",
      "  [195.]\n",
      "  [196.]\n",
      "  [197.]]\n",
      "\n",
      " [[198.]\n",
      "  [199.]\n",
      "  [200.]\n",
      "  [201.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [207.]\n",
      "  [208.]\n",
      "  [209.]\n",
      "  [210.]\n",
      "  [211.]\n",
      "  [212.]\n",
      "  [213.]\n",
      "  [214.]\n",
      "  [215.]]\n",
      "\n",
      " [[216.]\n",
      "  [217.]\n",
      "  [218.]\n",
      "  [219.]\n",
      "  [220.]\n",
      "  [221.]\n",
      "  [222.]\n",
      "  [223.]\n",
      "  [224.]\n",
      "  [225.]\n",
      "  [226.]\n",
      "  [227.]\n",
      "  [228.]\n",
      "  [229.]\n",
      "  [230.]\n",
      "  [231.]\n",
      "  [232.]\n",
      "  [233.]]\n",
      "\n",
      " [[234.]\n",
      "  [235.]\n",
      "  [236.]\n",
      "  [237.]\n",
      "  [238.]\n",
      "  [239.]\n",
      "  [240.]\n",
      "  [241.]\n",
      "  [242.]\n",
      "  [243.]\n",
      "  [244.]\n",
      "  [245.]\n",
      "  [246.]\n",
      "  [247.]\n",
      "  [248.]\n",
      "  [249.]\n",
      "  [250.]\n",
      "  [251.]]\n",
      "\n",
      " [[252.]\n",
      "  [253.]\n",
      "  [254.]\n",
      "  [255.]\n",
      "  [256.]\n",
      "  [257.]\n",
      "  [258.]\n",
      "  [259.]\n",
      "  [260.]\n",
      "  [261.]\n",
      "  [262.]\n",
      "  [263.]\n",
      "  [264.]\n",
      "  [265.]\n",
      "  [266.]\n",
      "  [267.]\n",
      "  [268.]\n",
      "  [269.]]\n",
      "\n",
      " [[270.]\n",
      "  [271.]\n",
      "  [272.]\n",
      "  [273.]\n",
      "  [274.]\n",
      "  [275.]\n",
      "  [276.]\n",
      "  [277.]\n",
      "  [278.]\n",
      "  [279.]\n",
      "  [280.]\n",
      "  [281.]\n",
      "  [282.]\n",
      "  [283.]\n",
      "  [284.]\n",
      "  [285.]\n",
      "  [286.]\n",
      "  [287.]]\n",
      "\n",
      " [[288.]\n",
      "  [289.]\n",
      "  [290.]\n",
      "  [291.]\n",
      "  [292.]\n",
      "  [293.]\n",
      "  [294.]\n",
      "  [295.]\n",
      "  [296.]\n",
      "  [297.]\n",
      "  [298.]\n",
      "  [299.]\n",
      "  [300.]\n",
      "  [301.]\n",
      "  [302.]\n",
      "  [303.]\n",
      "  [304.]\n",
      "  [305.]]\n",
      "\n",
      " [[306.]\n",
      "  [307.]\n",
      "  [308.]\n",
      "  [309.]\n",
      "  [310.]\n",
      "  [311.]\n",
      "  [312.]\n",
      "  [313.]\n",
      "  [314.]\n",
      "  [315.]\n",
      "  [316.]\n",
      "  [317.]\n",
      "  [318.]\n",
      "  [319.]\n",
      "  [320.]\n",
      "  [321.]\n",
      "  [322.]\n",
      "  [323.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr=np.empty((9,9,4))\n",
    "print(arr.shape)\n",
    "counter = 0\n",
    "\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        for k in range (4):\n",
    "            arr[i][j][k] = counter\n",
    "            counter += 1\n",
    "# print(arr)\n",
    "# imgResize = imgGray.reshape(64,64,64,1) # see imgResize\n",
    "arr = arr.reshape(18,18,1)\n",
    "print(arr.shape)\n",
    "print(arr[3][0][0])\n",
    "print(arr)\n",
    "\n",
    "\n",
    "#conclusion = reshape will reshape using index from right to left of arr[3rd][2nd][1st]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "0.0\n",
      "0.0\n",
      "[[[0.00000e+00 1.00000e+00 2.00000e+00 ... 6.10000e+01 6.20000e+01\n",
      "   6.30000e+01]\n",
      "  [6.40000e+01 6.50000e+01 6.60000e+01 ... 1.25000e+02 1.26000e+02\n",
      "   1.27000e+02]\n",
      "  [1.28000e+02 1.29000e+02 1.30000e+02 ... 1.89000e+02 1.90000e+02\n",
      "   1.91000e+02]\n",
      "  ...\n",
      "  [3.90400e+03 3.90500e+03 3.90600e+03 ... 3.96500e+03 3.96600e+03\n",
      "   3.96700e+03]\n",
      "  [3.96800e+03 3.96900e+03 3.97000e+03 ... 4.02900e+03 4.03000e+03\n",
      "   4.03100e+03]\n",
      "  [4.03200e+03 4.03300e+03 4.03400e+03 ... 4.09300e+03 4.09400e+03\n",
      "   4.09500e+03]]\n",
      "\n",
      " [[4.09600e+03 4.09700e+03 4.09800e+03 ... 4.15700e+03 4.15800e+03\n",
      "   4.15900e+03]\n",
      "  [4.16000e+03 4.16100e+03 4.16200e+03 ... 4.22100e+03 4.22200e+03\n",
      "   4.22300e+03]\n",
      "  [4.22400e+03 4.22500e+03 4.22600e+03 ... 4.28500e+03 4.28600e+03\n",
      "   4.28700e+03]\n",
      "  ...\n",
      "  [8.00000e+03 8.00100e+03 8.00200e+03 ... 8.06100e+03 8.06200e+03\n",
      "   8.06300e+03]\n",
      "  [8.06400e+03 8.06500e+03 8.06600e+03 ... 8.12500e+03 8.12600e+03\n",
      "   8.12700e+03]\n",
      "  [8.12800e+03 8.12900e+03 8.13000e+03 ... 8.18900e+03 8.19000e+03\n",
      "   8.19100e+03]]\n",
      "\n",
      " [[8.19200e+03 8.19300e+03 8.19400e+03 ... 8.25300e+03 8.25400e+03\n",
      "   8.25500e+03]\n",
      "  [8.25600e+03 8.25700e+03 8.25800e+03 ... 8.31700e+03 8.31800e+03\n",
      "   8.31900e+03]\n",
      "  [8.32000e+03 8.32100e+03 8.32200e+03 ... 8.38100e+03 8.38200e+03\n",
      "   8.38300e+03]\n",
      "  ...\n",
      "  [1.20960e+04 1.20970e+04 1.20980e+04 ... 1.21570e+04 1.21580e+04\n",
      "   1.21590e+04]\n",
      "  [1.21600e+04 1.21610e+04 1.21620e+04 ... 1.22210e+04 1.22220e+04\n",
      "   1.22230e+04]\n",
      "  [1.22240e+04 1.22250e+04 1.22260e+04 ... 1.22850e+04 1.22860e+04\n",
      "   1.22870e+04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.49856e+05 2.49857e+05 2.49858e+05 ... 2.49917e+05 2.49918e+05\n",
      "   2.49919e+05]\n",
      "  [2.49920e+05 2.49921e+05 2.49922e+05 ... 2.49981e+05 2.49982e+05\n",
      "   2.49983e+05]\n",
      "  [2.49984e+05 2.49985e+05 2.49986e+05 ... 2.50045e+05 2.50046e+05\n",
      "   2.50047e+05]\n",
      "  ...\n",
      "  [2.53760e+05 2.53761e+05 2.53762e+05 ... 2.53821e+05 2.53822e+05\n",
      "   2.53823e+05]\n",
      "  [2.53824e+05 2.53825e+05 2.53826e+05 ... 2.53885e+05 2.53886e+05\n",
      "   2.53887e+05]\n",
      "  [2.53888e+05 2.53889e+05 2.53890e+05 ... 2.53949e+05 2.53950e+05\n",
      "   2.53951e+05]]\n",
      "\n",
      " [[2.53952e+05 2.53953e+05 2.53954e+05 ... 2.54013e+05 2.54014e+05\n",
      "   2.54015e+05]\n",
      "  [2.54016e+05 2.54017e+05 2.54018e+05 ... 2.54077e+05 2.54078e+05\n",
      "   2.54079e+05]\n",
      "  [2.54080e+05 2.54081e+05 2.54082e+05 ... 2.54141e+05 2.54142e+05\n",
      "   2.54143e+05]\n",
      "  ...\n",
      "  [2.57856e+05 2.57857e+05 2.57858e+05 ... 2.57917e+05 2.57918e+05\n",
      "   2.57919e+05]\n",
      "  [2.57920e+05 2.57921e+05 2.57922e+05 ... 2.57981e+05 2.57982e+05\n",
      "   2.57983e+05]\n",
      "  [2.57984e+05 2.57985e+05 2.57986e+05 ... 2.58045e+05 2.58046e+05\n",
      "   2.58047e+05]]\n",
      "\n",
      " [[2.58048e+05 2.58049e+05 2.58050e+05 ... 2.58109e+05 2.58110e+05\n",
      "   2.58111e+05]\n",
      "  [2.58112e+05 2.58113e+05 2.58114e+05 ... 2.58173e+05 2.58174e+05\n",
      "   2.58175e+05]\n",
      "  [2.58176e+05 2.58177e+05 2.58178e+05 ... 2.58237e+05 2.58238e+05\n",
      "   2.58239e+05]\n",
      "  ...\n",
      "  [2.61952e+05 2.61953e+05 2.61954e+05 ... 2.62013e+05 2.62014e+05\n",
      "   2.62015e+05]\n",
      "  [2.62016e+05 2.62017e+05 2.62018e+05 ... 2.62077e+05 2.62078e+05\n",
      "   2.62079e+05]\n",
      "  [2.62080e+05 2.62081e+05 2.62082e+05 ... 2.62141e+05 2.62142e+05\n",
      "   2.62143e+05]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr=np.empty((512,512))\n",
    "print(arr.shape)\n",
    "counter = 0\n",
    "\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        arr[i][j] = int(counter)\n",
    "        counter += 1\n",
    "print(arr[0][0])\n",
    "arr = arr.reshape(64,64,64)\n",
    "print(arr[0][0][0])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 625, 3)\n",
      "(64, 64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (64, 64, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9c01c9f1aa90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResizedImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoise_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResizedImage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"output shape = \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# cropped = score[0,0:64,0:64,0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PoseEstimation\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PoseEstimation\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PoseEstimation\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (64, 64, 3)"
     ]
    }
   ],
   "source": [
    "#Testing out down sizing.\n",
    "# Question... will it crop or will it downsize\n",
    "# imgGray = cv2.resize(imgGray,(64,64))\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "img_path = \"./MyFaceCrop.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "# img = cv2.resize(img, None, fx=0.2, fy=0.2)interpolation = cv2.INTER_AREA\n",
    "ResizedImage = cv2.resize(img, (64,64) , interpolation = cv2.INTER_AREA)\n",
    "print(img.shape)\n",
    "print(ResizedImage.shape)\n",
    "cv2.imshow(\"3\", img)\n",
    "cv2.imshow(\"4\", ResizedImage)\n",
    "\n",
    "score = model.predict([noise_in, ResizedImage])\n",
    "print (\"output shape = \"+str(score.shape))\n",
    "# cropped = score[0,0:64,0:64,0]\n",
    "OUTPUT = score.reshape(64,64)\n",
    "print (\"output shape after = \"+str(OUTPUT.shape))\n",
    "# cropped = cv2.resize(cropped,(64,64))\n",
    "# OUTPUT = cv2.resize(OUTPUT,(600,600))\n",
    "\n",
    "#debug\n",
    "# print(noise_in)\n",
    "# cv2.imshow('3', cropped)\n",
    "cv2.imshow('sample image', OUTPUT)\n",
    "\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1_9:0' shape=(None, 16, 16, 1) dtype=float32>, <tf.Tensor 'input_2_9:0' shape=(None, 64, 64, 1) dtype=float32>]\n",
      "Tensor(\"conv2d_transpose_2_9/Sigmoid:0\", shape=(None, None, None, 1), dtype=float32)\n",
      "input shape = (64, 64, 3)\n",
      "noise(1, 16, 16, 1)\n",
      "output shape = (1, 64, 64, 1)\n",
      "output shape after = (64, 64)\n"
     ]
    }
   ],
   "source": [
    "##### sanitizer.h5 #####\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "# load model\n",
    "model = load_model('sanitizer_inv.h5')\n",
    "\n",
    "# show model i/o\n",
    "input = model.input\n",
    "output = model.output\n",
    "print (input)\n",
    "print (output)\n",
    "\n",
    "# summarize model.\n",
    "# model.summary()\n",
    "\n",
    "# def load_data():\n",
    "#     print(\"load_data()\\n\")\n",
    "#     d = loadmat('./UTKFace_Gray_AgeAbove3.mat')\n",
    "#     image, gender, age = d[\"image\"], d[\"gender\"][0], d[\"age\"][0]\n",
    "\n",
    "#     idx = np.where((age > 50) | (age < 20))[0]\n",
    "#     image = image[idx] / 255.0\n",
    "#     gender = gender[idx]\n",
    "#     age = age[idx]\n",
    "\n",
    "#     age = np.where(age > 30, 1, 0)\n",
    "#     age = np_utils.to_categorical(age, 2)\n",
    "#     gender = np_utils.to_categorical(gender, 2)\n",
    "\n",
    "#     return image, gender, age\n",
    "\n",
    "# # load dataset\n",
    "# image, gender, age = load_data()\n",
    "# img = image[201:202]\n",
    "\n",
    "img_path = \"./MyFaceCrop.jpg\"\n",
    "image = cv2.imread(img_path)\n",
    "# img = cv2.resize(img, None, fx=0.2, fy=0.2)interpolation = cv2.INTER_AREA\n",
    "imgResize = cv2.resize(image, (64,64) , interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "print(\"input shape = \"+str(imgResize.shape) )\n",
    "# imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"3\", imgResize)\n",
    "# print(\"shape\"+str(imgGray.shape))\n",
    "# imgGray = cv2.resize(imgGray,(64,64))\n",
    "# imgResize = imgGray.reshape(1,64,64,1) # see imgResize\n",
    "# print(\"resize\"+str(imgResize.shape))\n",
    "# cv2.imshow(\"1\", imgGray)\n",
    "imgGray = cv2.cvtColor(imgResize,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"5\", imgGray)\n",
    "img = imgGray.reshape(1,64,64,1)\n",
    "\n",
    "# cv2.imshow(\"4\", img)\n",
    "\n",
    "# create noise\n",
    "noise_in = np.random.normal(0, 1, (1, 16, 16, 1))\n",
    "# noise_pic = noise_in.reshape(22,22)\n",
    "# cv2.imshow(\"2\", noise_pic)\n",
    "\n",
    "print(\"noise\"+str(noise_in.shape))\n",
    "\n",
    "# evaluate the model\n",
    "score = model.predict([noise_in, img])\n",
    "print (\"output shape = \"+str(score.shape))\n",
    "# cropped = score[0,0:64,0:64,0]\n",
    "OUTPUT = score.reshape(64,64)\n",
    "print (\"output shape after = \"+str(OUTPUT.shape))\n",
    "# cropped = cv2.resize(cropped,(64,64))\n",
    "# OUTPUT = cv2.resize(OUTPUT,(600,600))\n",
    "\n",
    "#debug\n",
    "# print(noise_in)\n",
    "# cv2.imshow('3', cropped)\n",
    "cv2.imshow('sample image', OUTPUT)\n",
    "\n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
